{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135c432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure nltk is set up (run this once)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('taggers/averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9430fdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "#file_path = \"results/results_gradients/val-text-preds.csv\"\n",
    "#file_path = \"results/results_gradients/combined_val_text_preds.csv\"\n",
    "root_file_path = \"results/results_gradients/\"\n",
    "file_path_dict = {\n",
    "    \"ag_news\": root_file_path + \"ag_news_eval_output.csv\",\n",
    "    \"anthropic_toxic_prompts\": root_file_path + \"anthropic_toxic_prompts_eval_output.csv\",\n",
    "    \"arxiv\": root_file_path + \"arxiv_eval_output.csv\",\n",
    "#     \"one_million_instructions_train\": root_file_path + \"one_million_instructions_train_1000_eval_output.csv\",\n",
    "      \"one_million_instructions_train_10k\": root_file_path + \"one_million_instructions_train_10k_eval_output.csv\",\n",
    "    \"one_million_instructions_val\": root_file_path + \"one_million_instructions_val_eval_output.csv\",\n",
    "    \"wikibio\": root_file_path + \"wikibio_eval_output.csv\",\n",
    "    \"xsum_doc\": root_file_path + \"xsum_doc_eval_output.csv\",\n",
    "    \"xsum_summ\": root_file_path + \"xsum_summ_eval_output.csv\",\n",
    "    \"python_code_alpaca\": root_file_path + \"python_code_alpaca_eval_output.csv\"\n",
    "}\n",
    "df_dict = {}\n",
    "for key, value in file_path_dict.items():\n",
    "    print(f\"reading {key}'s file\", flush=True)\n",
    "    df_dict[key] = pd.read_csv(value)\n",
    "\n",
    "# Function to calculate matching token length\n",
    "def calculate_matching_length(row):\n",
    "    original_tokens = word_tokenize(row['Original'])\n",
    "    decoded_tokens = word_tokenize(row['Decoded'])\n",
    "    correct_tokens = [tok for tok, dec_tok in zip(original_tokens, decoded_tokens) if tok == dec_tok]\n",
    "    return len(correct_tokens), len(original_tokens)\n",
    "\n",
    "# for key, df in df_dict.items():\n",
    "#     # Apply function to each row\n",
    "#     df[['correct_token_length', 'original_token_length']] = df.apply(\n",
    "#         lambda row: pd.Series(calculate_matching_length(row)), axis=1\n",
    "#     )\n",
    "#     # Calculate additional stats if needed\n",
    "#     df['match_ratio'] = df['correct_token_length'] / df['original_token_length']\n",
    "\n",
    "#     # Save to a new CSV or inspect\n",
    "#     output_path = key + \"_comparison_results.csv\"\n",
    "#     df.to_csv(output_path, index=False)\n",
    "#     print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51300809-48ca-47af-bfec-83ac54938428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.tree import Tree\n",
    "\n",
    "# Function to extract person names using NER\n",
    "def extract_person_names(sentence):\n",
    "    if not isinstance(sentence, str) or sentence.strip() == \"\":\n",
    "        return set()\n",
    "    chunked = ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "    person_names = set()\n",
    "    for chunk in chunked:\n",
    "        if isinstance(chunk, Tree) and chunk.label() == 'PERSON':\n",
    "            name = \" \".join(c[0] for c in chunk)\n",
    "            person_names.add(name)\n",
    "    return person_names\n",
    "\n",
    "\n",
    "# for key, data in df_dict.items():\n",
    "# # Extract person names from both columns\n",
    "#     output_file_path = key + \"_person_instances.csv\"\n",
    "#     data['Original_Names'] = data['Original'].apply(extract_person_names)\n",
    "#     data['Decoded_Names'] = data['Decoded'].apply(extract_person_names)\n",
    "#     data[['Original', 'Decoded', 'Original_Names', 'Decoded_Names']].to_csv(output_file_path, index=False)\n",
    "    \n",
    "#     # Calculate precision and recall\n",
    "#     true_positive = 0\n",
    "#     total_original_names = 0\n",
    "#     total_decoded_names = 0\n",
    "    \n",
    "#     for _, row in data.iterrows():\n",
    "#         original_names = row['Original_Names']\n",
    "#         decoded_names = row['Decoded_Names']\n",
    "        \n",
    "#         true_positive += len(original_names & decoded_names)\n",
    "#         total_original_names += len(original_names)\n",
    "#         total_decoded_names += len(decoded_names)\n",
    "    \n",
    "#     precision = true_positive / total_decoded_names if total_decoded_names > 0 else 0\n",
    "#     recall = true_positive / total_original_names if total_original_names > 0 else 0\n",
    "    \n",
    "#     # Output the results\n",
    "#     print(f\"Precision: {precision:.2f}\")\n",
    "#     print(f\"Recall: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "475b06c8-be4c-4835-b70a-bfd87bea052e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/u/koyena/miniconda3/envs/engine/lib/python3.8/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Matrix:\n",
      "[[1.         0.9315515  0.78193676 0.80356836 0.73167723 0.67972594]\n",
      " [0.9315515  1.         0.7944467  0.72048306 0.7646935  0.62692446]\n",
      " [0.78193676 0.7944467  1.0000001  0.6352874  0.60976624 0.5672598 ]\n",
      " [0.80356836 0.72048306 0.6352874  1.0000001  0.9181043  0.7095541 ]\n",
      " [0.73167723 0.7646935  0.60976624 0.9181043  1.0000002  0.6342138 ]\n",
      " [0.67972594 0.62692446 0.5672598  0.7095541  0.6342138  0.9999999 ]]\n",
      "\n",
      "Formatted Cosine Similarity Matrix:\n",
      "[[1.   0.93 0.78 0.8  0.73 0.68]\n",
      " [0.93 1.   0.79 0.72 0.76 0.63]\n",
      " [0.78 0.79 1.   0.64 0.61 0.57]\n",
      " [0.8  0.72 0.64 1.   0.92 0.71]\n",
      " [0.73 0.76 0.61 0.92 1.   0.63]\n",
      " [0.68 0.63 0.57 0.71 0.63 1.  ]]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Define the sentences\n",
    "sentences = [\n",
    "    \"Is Bob coming to the meeting today?\",\n",
    "    \"Bobâ€™s coming to the meeting today?\",\n",
    "    \"The meeting attendance scheduled for today includes Bob, correct?\",\n",
    "    \"Is Bob going to the concert today?\",\n",
    "    \"Bob's coming to the concert today?\",\n",
    "    \"Bob will be playing today, right?\"\n",
    "]\n",
    "\n",
    "# Load a pre-trained Sentence-BERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # You can choose other Sentence-BERT models as well\n",
    "\n",
    "# Generate embeddings for each sentence\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# Compute pairwise cosine similarities\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# Print the cosine similarity matrix\n",
    "print(\"Cosine Similarity Matrix:\")\n",
    "print(similarity_matrix)\n",
    "\n",
    "# Optional: Format the matrix for better readability\n",
    "formatted_matrix = np.round(similarity_matrix, 2)\n",
    "print(\"\\nFormatted Cosine Similarity Matrix:\")\n",
    "print(formatted_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c030ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# for key, df in df_dict.items():\n",
    "#     print(f\"KEY: {key}\")\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     sns.regplot(x='original_token_length', y='correct_token_length', data=df, scatter_kws={'alpha':0.6}, line_kws={'color':'red'})\n",
    "#     plt.title('Trend of Original vs Correct Token Lengths', fontsize=14)\n",
    "#     plt.xlabel('Original Token Length', fontsize=12)\n",
    "#     plt.ylabel('Correct Token Length', fontsize=12)\n",
    "#     plt.grid(True)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f37793",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in df_dict.items():\n",
    "    print(f\"KEY: {key}\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df['original_token_length'], df['correct_token_length'], alpha=0.6, color='b', label=\"Correct Tokens\")\n",
    "    plt.plot([0, max(df['original_token_length'])], [0, max(df['original_token_length'])], color='r', linestyle='--', label=\"Perfect Match\")\n",
    "\n",
    "    plt.title('Comparison of Original and Correctly Decoded Token Lengths', fontsize=14)\n",
    "    plt.xlabel('Original Token Length', fontsize=12)\n",
    "    plt.ylabel('Correct Token Length', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998689c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms\n",
    "for key, df in df_dict.items():\n",
    "    print(f\"KEY: {key}\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(df['original_token_length'], bins=20, alpha=0.6, label='Original Token Length', color='blue')\n",
    "    plt.hist(df['correct_token_length'], bins=20, alpha=0.6, label='Correct Token Length', color='orange')\n",
    "\n",
    "    # Add labels and legend\n",
    "    plt.title('Distribution of Token Lengths', fontsize=14)\n",
    "    plt.xlabel('Token Length', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343579e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the ratio of correct to original token lengths\n",
    "for key, df in df_dict.items():\n",
    "    print(f\"KEY: {key}\")\n",
    "    df['ratio'] = df['correct_token_length'] / df['original_token_length']\n",
    "\n",
    "    # Create heatmap data: ratio values as a 2D array with one row\n",
    "    heatmap_data = np.expand_dims(df['ratio'].values, axis=0)\n",
    "\n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(15, 2))  # Wide and short for a compact representation\n",
    "    sns.heatmap(heatmap_data, cmap=\"coolwarm\", cbar=True, annot=False, xticklabels=False, yticklabels=['Ratio'])\n",
    "\n",
    "    # Add labels\n",
    "    plt.title('Heatmap of Correct to Original Token Length Ratios', fontsize=14)\n",
    "    plt.xlabel('Sentence Index', fontsize=12)\n",
    "    plt.ylabel('Correctness Ratio', fontsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782ddded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# file_path = \"results/results-gradients/val-text-preds.csv\"\n",
    "# df = pd.read_csv(file_path)\n",
    "\n",
    "# Load a pre-trained model from SentenceTransformers\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # You can choose other models as needed\n",
    "tokenizer = model.tokenizer\n",
    "for key, df in df_dict.items():\n",
    "    print(f\"KEY: {key}\")\n",
    "    # Compute embeddings\n",
    "    embeddings1 = model.encode(df[\"Original\"].tolist(), convert_to_tensor=True)\n",
    "    embeddings2 = model.encode(df[\"Decoded\"].tolist(), convert_to_tensor=True)\n",
    "\n",
    "    # Compute token lengths for each sentence\n",
    "    df[\"tokens_original\"] = df[\"Original\"].apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "    df[\"tokens_decoded\"] = df[\"Decoded\"].apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarities = cosine_similarity(embeddings1.cpu().numpy(), embeddings2.cpu().numpy())\n",
    "    diagonal_similarities = np.diagonal(similarities)  # Get pairwise similarities\n",
    "\n",
    "    # Add similarity scores to the DataFrame\n",
    "    df[\"similarity\"] = diagonal_similarities\n",
    "\n",
    "    # Sort the DataFrame by similarity in descending order\n",
    "    df_sorted = df.sort_values(by=\"similarity\", ascending=False)\n",
    "\n",
    "    # Display results\n",
    "    print(\"Sentence pairs sorted by similarity (most to least):\")\n",
    "    for _, row in df_sorted.iterrows():\n",
    "        print(f\"Pair: ({row['Original']}, {row['Decoded']})\\nSimilarity: {row['similarity']:.4f}\\n\")\n",
    "\n",
    "    df.to_csv(\"cos_sim_\" + key + \"_eval.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5763f34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Calculate length difference and add to the DataFrame\n",
    "# for key, df in df_dict.items():\n",
    "#     print(f\"KEY: {key}\")\n",
    "#     df_sorted = df.sort_values(by=\"similarity\", ascending=False)\n",
    "#     df_sorted[\"length\"] = df_sorted[\"tokens_decoded\"]\n",
    "\n",
    "#     # Plot similarity vs. length difference\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.scatter(\n",
    "#         df_sorted[\"length\"], \n",
    "#         df_sorted[\"similarity\"], \n",
    "#         c=df_sorted[\"similarity\"], \n",
    "#         cmap=\"viridis\", \n",
    "#         edgecolor=\"k\", \n",
    "#         s=100\n",
    "#     )\n",
    "\n",
    "#     # Add labels and title\n",
    "#     plt.title(\"Similarity vs. Decoded Token Length\", fontsize=14)\n",
    "#     plt.xlabel(\"Token Length\", fontsize=12)\n",
    "#     plt.ylabel(\"Similarity Score\", fontsize=12)\n",
    "\n",
    "#     # Add color bar for similarity\n",
    "#     cbar = plt.colorbar()\n",
    "#     cbar.set_label(\"Similarity Score\", fontsize=12)\n",
    "\n",
    "#     # Show grid and plot\n",
    "#     plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920d6fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate length difference and add to the DataFrame\n",
    "for key, df in df_dict.items():\n",
    "    print(f\"KEY: {key}\")\n",
    "    df_sorted = df.sort_values(by=\"similarity\", ascending=False)\n",
    "    df['length_bin'] = pd.cut(df['tokens_decoded'], bins=20)\n",
    "    binned_data = df.groupby('length_bin')['similarity'].mean().reset_index()\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(\n",
    "        x=binned_data['length_bin'].astype(str), \n",
    "        y=binned_data['similarity'], \n",
    "        palette='Blues'\n",
    "    )\n",
    "\n",
    "    plt.title('Binned Cosine Similarity vs Original Token Length', fontsize=14)\n",
    "    plt.xlabel('Original Token Length (Binned)', fontsize=12)\n",
    "    plt.ylabel('Average Cosine Similarity', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d3cb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Determine the global range of 'original_token_length'\n",
    "all_lengths = pd.concat([df['original_token_length'] for df in df_dict.values()])\n",
    "global_min, global_max = all_lengths.min(), all_lengths.max()\n",
    "\n",
    "# Step 2: Create consistent bins\n",
    "bins = np.linspace(global_min, global_max, 20)  # 20 bins for example\n",
    "bin_labels = [f\"{int(bins[i])}-{int(bins[i+1])}\" for i in range(len(bins)-1)]\n",
    "\n",
    "# Step 3: Apply binning to each DataFrame and compute aggregates\n",
    "binned_data_cosine = []\n",
    "binned_data_decoded = []\n",
    "\n",
    "for dataset_name, df in df_dict.items():\n",
    "    # Binning\n",
    "    df['length_bin'] = pd.cut(df['tokens_original'], bins=bins, labels=bin_labels, include_lowest=True)\n",
    "    \n",
    "    # Aggregating cosine similarity\n",
    "    binned_cosine = df.groupby('length_bin')['similarity'].mean().reset_index()\n",
    "    binned_cosine['dataset'] = dataset_name\n",
    "    binned_data_cosine.append(binned_cosine)\n",
    "    \n",
    "    # Aggregating decoded token length\n",
    "    binned_decoded = df.groupby('length_bin')['tokens_decoded'].mean().reset_index()\n",
    "    binned_decoded['dataset'] = dataset_name\n",
    "    binned_data_decoded.append(binned_decoded)\n",
    "\n",
    "# Combine the results\n",
    "combined_cosine_data = pd.concat(binned_data_cosine, ignore_index=True)\n",
    "combined_decoded_data = pd.concat(binned_data_decoded, ignore_index=True)\n",
    "\n",
    "# Step 4: Plot the trends\n",
    "\n",
    "# Cosine Similarity Trends\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(\n",
    "    data=combined_cosine_data,\n",
    "    x='length_bin', y='similarity', hue='dataset',\n",
    "    marker='o'\n",
    ")\n",
    "plt.title('Cosine Similarity Trends Across Token Length Bins', fontsize=14)\n",
    "plt.xlabel('Original Token Length (Binned)', fontsize=12)\n",
    "plt.ylabel('Average Cosine Similarity', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Dataset')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Decoded Token Length Trends\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(\n",
    "    data=combined_decoded_data,\n",
    "    x='length_bin', y='tokens_decoded', hue='dataset',\n",
    "    marker='o'\n",
    ")\n",
    "plt.title('Decoded Token Length Trends Across Original Token Length Bins', fontsize=14)\n",
    "plt.xlabel('Original Token Length (Binned)', fontsize=12)\n",
    "plt.ylabel('Average Decoded Token Length', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Dataset')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a5f968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# Step 1: Determine the global range of 'original_token_length'\n",
    "all_lengths = pd.concat([df['tokens_original'] for df in df_dict.values()])\n",
    "global_min, global_max = all_lengths.min(), all_lengths.max()\n",
    "\n",
    "# Step 2: Create consistent bins\n",
    "bins = np.linspace(global_min, global_max, 20)  # 20 bins for example\n",
    "bin_labels = [f\"{int(bins[i])}-{int(bins[i+1])}\" for i in range(len(bins)-1)]\n",
    "\n",
    "# Step 3: Add BLEU scores and apply binning\n",
    "binned_data_bleu = []\n",
    "binned_data_cosine = []\n",
    "binned_data_decoded = []\n",
    "\n",
    "for dataset_name, df in df_dict.items():\n",
    "    # Adding BLEU scores\n",
    "    df['bleu_score'] = df.apply(\n",
    "        lambda row: sentence_bleu(\n",
    "            [[str(row['Original'])]],  # Reference sequence\n",
    "            [str(row['Decoded'])],    # Candidate sequence\n",
    "            weights=(1.0,)  # BLEU-1\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Binning\n",
    "    df['length_bin'] = pd.cut(df['tokens_original'], bins=bins, labels=bin_labels, include_lowest=True)\n",
    "    \n",
    "    # Aggregating BLEU scores\n",
    "    binned_bleu = df.groupby('length_bin')['bleu_score'].mean().reset_index()\n",
    "    binned_bleu['dataset'] = dataset_name\n",
    "    binned_data_bleu.append(binned_bleu)\n",
    "\n",
    "# Combine the results\n",
    "combined_bleu_data = pd.concat(binned_data_bleu, ignore_index=True)\n",
    "# Step 4: Plot the trends\n",
    "\n",
    "# BLEU Score Trends\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(\n",
    "    data=combined_bleu_data,\n",
    "    x='length_bin', y='bleu_score', hue='dataset',\n",
    "    marker='o'\n",
    ")\n",
    "plt.title('BLEU Score Trends Across Token Length Bins', fontsize=14)\n",
    "plt.xlabel('Original Token Length (Binned)', fontsize=12)\n",
    "plt.ylabel('Average BLEU Score', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Dataset')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d07c3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for key, df in df_dict.items():\n",
    "    print(f\"KEY: {key}\")\n",
    "    df_sorted = df.sort_values(by=\"similarity\", ascending=False)\n",
    "    df_sorted[\"length\"] = df_sorted[\"tokens_original\"]\n",
    "    # Aggregate similarity by exact length difference values\n",
    "    heatmap_data = df_sorted.groupby(\"length\")[\"similarity\"].mean().reset_index()\n",
    "\n",
    "    # Replace NaN with 0\n",
    "    heatmap_data[\"similarity\"] = heatmap_data[\"similarity\"].fillna(0)\n",
    "\n",
    "    # Prepare data for the heatmap\n",
    "    heatmap_pivot = heatmap_data.pivot_table(columns=\"length\", values=\"similarity\")  # Pivot for horizontal axis\n",
    "\n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(\n",
    "        heatmap_pivot, \n",
    "        cmap=\"viridis\", \n",
    "        cbar_kws={\"label\": \"Average Similarity\"},\n",
    "        linewidths=0.5,\n",
    "        fmt=\".2f\"\n",
    "    )\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.title(\"Heatmap of Similarity vs. Token Length\", fontsize=14)\n",
    "    plt.xlabel(\"Token Length\", fontsize=12)\n",
    "    plt.ylabel(\"Average Similarity\", fontsize=12)\n",
    "\n",
    "    # Rotate x-axis labels for better visibility\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930b91c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for key, df in df_dict.items():\n",
    "    print(f\"KEY: {key}\")\n",
    "    # Tokenize to calculate lengths\n",
    "    original_lengths = df[\"tokens_original\"]\n",
    "    decoded_lengths = df[\"tokens_decoded\"]\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(original_lengths, decoded_lengths, alpha=0.6, edgecolor='k', label=\"Token Lengths\")\n",
    "    plt.plot([0, max(original_lengths + decoded_lengths)], [0, max(original_lengths + decoded_lengths)], \n",
    "             color=\"red\", linestyle=\"--\", label=\"y=x (ideal)\")\n",
    "\n",
    "    plt.title(\"Original Token Length vs. Decoded Token Length\")\n",
    "    plt.xlabel(\"Original Token Length\")\n",
    "    plt.ylabel(\"Decoded Token Length\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3401531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce09ada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238a270e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "engine",
   "language": "python",
   "name": "engine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
